{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"00_EveryCode_starting.ipynb\"\n",
    "\n",
    "Huge_data_path = '../huge data source/AE/'\n",
    "Package_path = Huge_data_path + 'package/'\n",
    "dataset_list = ['baseset_withdroped','single_lagset','avgstatset','slopeset','PCAset','NAset','cat_WoE_baseset','Woe_slopeset','time_WoE_baseset','refeature_items','acfpacf','LmmMean',]#time\n",
    "catset_list = ['cat_stat','time_stat',]#time\n",
    "ignore_list = []#['prop']\n",
    "all_orgitems = [\n",
    " 'B_1', 'B_10', 'B_11', 'B_12', 'B_13', 'B_14', 'B_15', 'B_16', 'B_17', 'B_18', 'B_19', 'B_2', 'B_20', 'B_21', 'B_22', 'B_23', 'B_24', 'B_25', 'B_26', 'B_27', 'B_28', 'B_29', 'B_3', 'B_30',\n",
    " 'B_31', 'B_32', 'B_33', 'B_36', 'B_37', 'B_38', 'B_39', 'B_4', 'B_40', 'B_41', 'B_42', 'B_5', 'B_6', 'B_7', 'B_8', 'B_9', \n",
    " 'D_102',  'D_104', 'D_105', 'D_106', 'D_107', 'D_108', 'D_109', 'D_110', 'D_111', 'D_112', 'D_113', 'D_114', 'D_115', 'D_116', 'D_117', 'D_118', 'D_119', 'D_120',\n",
    " 'D_121', 'D_122', 'D_123', 'D_124', 'D_125', 'D_126', 'D_127', 'D_128', 'D_129', 'D_130', 'D_131', 'D_132', 'D_133', 'D_134', 'D_135', 'D_136', 'D_137', 'D_138', \n",
    " 'D_140', 'D_141', 'D_142', 'D_143', 'D_144', 'D_145', \n",
    " 'D_39', 'D_41', 'D_42', 'D_43', 'D_44', 'D_45', 'D_46', 'D_47', 'D_48', 'D_49', 'D_50', 'D_51', 'D_52', 'D_53', 'D_54', 'D_55', 'D_56', 'D_58', 'D_59', 'D_60',\n",
    " 'D_61', 'D_62', 'D_63', 'D_64', 'D_65', 'D_66', 'D_68', 'D_69', 'D_70', 'D_71', 'D_72', 'D_73', 'D_74', 'D_75', 'D_76', 'D_77', 'D_78', 'D_79', \n",
    " 'D_80', 'D_81', 'D_82', 'D_83', 'D_84', 'D_86', 'D_87', 'D_88', 'D_89', 'D_91', 'D_92', 'D_93', 'D_94', 'D_96',\n",
    " 'P_2', 'P_3', 'P_4',\n",
    " 'R_1', 'R_10', 'R_11', 'R_12', 'R_13', 'R_14', 'R_15', 'R_16', 'R_17', 'R_18', 'R_19', 'R_2', 'R_20', 'R_21', 'R_22', 'R_23', 'R_24', 'R_25', 'R_26', 'R_27', 'R_28', \n",
    " 'R_3', 'R_4', 'R_5', 'R_6', 'R_7', 'R_8', 'R_9', \n",
    " 'S_11', 'S_12', 'S_13', 'S_15', 'S_16', 'S_17', 'S_18', 'S_19', 'S_20', 'S_22', 'S_23', 'S_24', 'S_25', 'S_26', 'S_27', 'S_3', 'S_5', 'S_6', 'S_7', 'S_8', 'S_9'\n",
    " ]#'D_103','D_139']\n",
    "\n",
    " \n",
    "cat_list = list(['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) \n",
    "bincat_list = list(['D_114','D_116','D_120','D_66']) \n",
    "multcat_list = [col for col in cat_list if col not in bincat_list]\n",
    "num_list = [col for col in all_orgitems if col not in cat_list]\n",
    "num_list.sort()\n",
    "time_list = ['S_2_dt_wd','S_2_dt_m_in3']\n",
    "\n",
    "NA_review_list = ['D_103','S_3','D_59','D_126','D_46','S_25','R_27','S_27','D_91','D_43','D_113','D_55','S_24','S_22','D_69','D_68',\n",
    "'D_64','D_74','B_8','D_72','D_77','D_81','D_44','S_9','D_52','D_56','P_3','D_105','B_17','D_50','D_82','D_140','D_102','D_144','D_53',\n",
    "'D_133','D_142','P_2','D_42','D_66','D_76','R_26','B_13','D_49','D_132','D_106','B_29','R_9','D_134','B_42','D_73','B_39','D_110','D_108',\n",
    "'D_88','D_87','D_61','D_48','D_62','D_79','D_70']\n",
    "\n",
    "varible_predrop_list = [\n",
    "       #due to large NA count >80% of full set\n",
    "       'D_88', 'D_108', 'D_111', 'D_110', 'B_39', 'D_73', 'B_42', #'D_87' for fill 0\n",
    "       'D_134', 'D_135', 'D_136', 'D_137', 'D_138', 'R_9', 'B_29', 'D_106',\n",
    "       'D_132', 'D_49', 'R_26', 'D_76', 'D_66', 'D_42', 'D_142', 'D_53',\n",
    "       'D_82',\n",
    "       #due to similar varible\n",
    "       'D_103','D_139']\n",
    "#used_items = [c for c in all_orgitems if c not in varible_predrop_list]\n",
    "#for data gen propose\n",
    "cat_list_inModel = [c for c in cat_list if c not in varible_predrop_list]\n",
    "num_list_inModel = [c for c in num_list if c not in varible_predrop_list]\n",
    "\n",
    "Centreize_items = ['D_55','D_56','D_60','D_61','S_7',]\n",
    "Standardize_items = ['D_73','D_76','R_9','S_5','S_8','S_9',]\n",
    "Slope_items = ['B_1','B_10','B_11','B_12','B_14','B_16','B_17','B_18','B_19','B_2','B_20','B_22','B_23','B_25','B_26','B_28','B_3','B_33',\n",
    "'B_37','B_39','B_4','B_40','B_42','B_5','B_6','B_7','B_9','D_112','D_124','D_135','D_39','D_41','D_42','D_43','D_44','D_48','D_53','D_55','D_58',\n",
    "'D_61','D_62','D_65','D_70','D_74','D_75','D_76','D_77','D_78','D_80','D_84','P_2','P_3','R_1','R_10','R_11','R_15','R_16','R_2','R_3','R_4','R_5',\n",
    "'R_6','R_8','S_13','S_23','S_25','S_27','S_3','S_5','S_6','S_7','S_8',]\n",
    "fill_0 = ['D_87',]\n",
    "Q50diff = ['D_59',]\n",
    "Q50diff2 = ['S_11','R_7','B_17','D_46']\n",
    "\n",
    "pacf_set = ['B_1','B_2','B_3','B_4','B_7','B_9','B_11','B_12','B_14','B_16','B_17','B_19','B_20','B_22','B_23','B_25','B_28','B_37','B_42','D_41','D_42','D_44','D_46','D_46_Q50std','D_48','D_53','D_55','D_58','D_59_Q50std','D_70','D_73_std','D_74','D_75','P_2','P_3','P_4','R_1','R_2','S_5','S_5_std','S_23','S_24']\n",
    "acf_set = ['B_1','B_2','B_3','B_4','B_7','B_9','B_11','B_12','B_14','B_16','B_17','B_18','B_19','B_20','B_22','B_23','B_25','B_28','B_31','B_37','B_40','B_42','D_41','D_42','D_44','D_46','D_46_Q50std','D_48','D_53','D_55','D_58','D_59_Q50std','D_60','D_70','D_73_std','D_74','D_75','D_76','D_112','D_113','P_2','P_3','P_4','R_1','R_2','R_3','S_5','S_5_std','S_6','S_22','S_23','S_24','S_25','S_27']\n",
    "\n",
    "fix_item = ['period_count','S_2_dt_mean2last']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function: load_DProcessed_df(file), amex_metric(y_true, y_pred), file_get(dataset,version,dataset_list,num_list_inModel,ignore_list)\n",
      "list:all_cols,cat_list,num_list , varible: Huge_data_path\n"
     ]
    }
   ],
   "source": [
    "def amex_metric(y_true, y_pred):\n",
    "    labels = np.transpose(np.array([y_true, y_pred]))\n",
    "    labels = labels[labels[:, 1].argsort()[::-1]]\n",
    "    weights = np.where(labels[:,0]==0, 20, 1)\n",
    "    cut_vals = labels[np.cumsum(weights) <= int(0.04 * np.sum(weights))]\n",
    "    top_four = np.sum(cut_vals[:,0]) / np.sum(labels[:,0])\n",
    "    gini = [0,0]\n",
    "    for i in [1,0]:\n",
    "        labels = np.transpose(np.array([y_true, y_pred]))\n",
    "        labels = labels[labels[:, i].argsort()[::-1]]\n",
    "        weight = np.where(labels[:,0]==0, 20, 1)\n",
    "        weight_random = np.cumsum(weight / np.sum(weight))\n",
    "        total_pos = np.sum(labels[:, 0] *  weight)\n",
    "        cum_pos_found = np.cumsum(labels[:, 0] * weight)\n",
    "        lorentz = cum_pos_found / total_pos\n",
    "        gini[i] = np.sum((lorentz - weight_random) * weight)\n",
    "    return gini[1]/gini[0], top_four, 0.5 * (gini[1]/gini[0] + top_four)*100,cut_vals[:,1].min()\n",
    "\n",
    "def load_DProcessed_df(file):\n",
    "    global Huge_data_path\n",
    "    data_df = pd.read_feather(Huge_data_path+file+'.ftr')\n",
    "    data_df[cat_list] = data_df[cat_list].astype('category')\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def file_get(dataset,version,dataset_list,num_list_inModel,ignore_list,catset_list = []):\n",
    "    global Package_path,Huge_data_path\n",
    "    data_df = pd.read_pickle(Huge_data_path+dataset+'_labels.pkl').set_index('customer_ID')\n",
    "\n",
    "    for file in dataset_list+catset_list:\n",
    "        try:\n",
    "            if file in dataset_list:\n",
    "                temp_df = pd.read_feather(Package_path + file +'_'+dataset+'.ftr').set_index('customer_ID')\n",
    "            else:\n",
    "                temp_df = pd.read_pickle(Package_path + file +'_'+dataset+'.plk')\n",
    "        except:\n",
    "            print('missing file:' + Package_path + file +'_'+dataset+'.ftr')\n",
    "            continue\n",
    "        needed_item = [col for col in temp_df.columns if \n",
    "        (col.startswith(tuple([str(col) + '_' for col in num_list_inModel])))and #logic that contain items\n",
    "        (all(ignore not in col for ignore in ignore_list))                      #logic that skip ignore str\n",
    "        ]\n",
    "        data_df = data_df.join(temp_df[needed_item], on = 'customer_ID')\n",
    "    return data_df\n",
    "\n",
    "def file_getII(dataset,version,dataset_list,needed_item,catset_list = []):\n",
    "    global Package_path,Huge_data_path\n",
    "    data_df = pd.read_pickle(Huge_data_path+dataset+'_labels.pkl').set_index('customer_ID')\n",
    "\n",
    "    for file in dataset_list+catset_list:\n",
    "        try:\n",
    "            if file in dataset_list:\n",
    "                temp_df = pd.read_feather(Package_path + file +'_'+dataset+'.ftr').set_index('customer_ID')\n",
    "            else:\n",
    "                temp_df = pd.read_pickle(Package_path + file +'_'+dataset+'.plk')\n",
    "        except:\n",
    "            print('missing file:' + Package_path + file +'_'+dataset+'.ftr')\n",
    "            continue\n",
    "        get_item = [col for col in temp_df.columns if col in needed_item]\n",
    "        data_df = data_df.join(temp_df[get_item], on = 'customer_ID')\n",
    "    return data_df\n",
    "\n",
    "def file_get_word(dataset,version,dataset_list,word_contain,catset_list = []):\n",
    "    global Package_path,Huge_data_path\n",
    "    data_df = pd.read_pickle(Huge_data_path+dataset+'_labels.pkl').set_index('customer_ID')\n",
    "\n",
    "    for file in dataset_list+catset_list:\n",
    "        try:\n",
    "            if file in dataset_list:\n",
    "                temp_df = pd.read_feather(Package_path + file +'_'+dataset+'.ftr').set_index('customer_ID')\n",
    "            else:\n",
    "                temp_df = pd.read_pickle(Package_path + file +'_'+dataset+'.plk')\n",
    "        except:\n",
    "            print('missing file:' + Package_path + file +'_'+dataset+'.ftr')\n",
    "            continue\n",
    "        get_item = [col for col in temp_df.columns if all(word in col for word in word_contain)]\n",
    "        #print(get_item)\n",
    "        data_df = data_df.join(temp_df[get_item], on = 'customer_ID')\n",
    "    return data_df\n",
    "\n",
    "print('function: load_DProcessed_df(file), amex_metric(y_true, y_pred), file_get(dataset,version,dataset_list,num_list_inModel,ignore_list)')\n",
    "print('list:all_cols,cat_list,num_list , varible: Huge_data_path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Lam\\Documents\\GitHub\\AX-project\\10_Data into model format.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Lam/Documents/GitHub/AX-project/10_Data%20into%20model%20format.ipynb#W2sZmlsZQ%3D%3D?line=166'>167</a>\u001b[0m     cat_agg\u001b[39m.\u001b[39mreset_index(inplace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Lam/Documents/GitHub/AX-project/10_Data%20into%20model%20format.ipynb#W2sZmlsZQ%3D%3D?line=167'>168</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m cat_agg\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Lam/Documents/GitHub/AX-project/10_Data%20into%20model%20format.ipynb#W2sZmlsZQ%3D%3D?line=171'>172</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata_process_PCA\u001b[39m(data_df,num_list,n_feature,pca\u001b[39m=\u001b[39mPCA()):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Lam/Documents/GitHub/AX-project/10_Data%20into%20model%20format.ipynb#W2sZmlsZQ%3D%3D?line=172'>173</a>\u001b[0m     \u001b[39m#PCA feature #train need to apply same pca to test!\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Lam/Documents/GitHub/AX-project/10_Data%20into%20model%20format.ipynb#W2sZmlsZQ%3D%3D?line=173'>174</a>\u001b[0m     \u001b[39mif\u001b[39;00m pca \u001b[39m==\u001b[39m PCA():\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Lam/Documents/GitHub/AX-project/10_Data%20into%20model%20format.ipynb#W2sZmlsZQ%3D%3D?line=174'>175</a>\u001b[0m         pca \u001b[39m=\u001b[39m PCA(n_feature)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def data_process_DTpara(data_df):\n",
    "    \n",
    "    return data_df\n",
    "\n",
    "def data_process_loadfiles(dataset):\n",
    "    global Huge_data_path\n",
    "    label_df = pd.read_pickle(Huge_data_path+dataset+'_labels.pkl')\n",
    "    data_df =  pd.read_feather(Huge_data_path+dataset+'_data.ftr')\n",
    "    data_df=data_df.join(label_df.set_index('customer_ID'), on='customer_ID') #no target for [test]\n",
    "    return data_df,label_df\n",
    "\n",
    "def data_process_baseUpdate(data_df):\n",
    "    data_df = data_df[pd.DataFrame(data_df.columns).sort_values(0)[0]]\n",
    "\n",
    "    data_df[cat_list] = data_df[cat_list].astype(\"category\")\n",
    "\n",
    "    data_df['S_2_dt'] =  pd.to_datetime(data_df['S_2'],format=\"%Y-%m-%d\")\n",
    "    data_df['S_2_dt_d']=data_df['S_2_dt'].dt.day\n",
    "    data_df['S_2_dt_m']=data_df['S_2_dt'].dt.month\n",
    "    data_df['S_2_dt_y']=data_df['S_2_dt'].dt.year\n",
    "    data_df['S_2_dt_wd']=data_df['S_2_dt'].dt.weekday\n",
    "    data_df['S_2_dt_sm']=data_df['S_2_dt_d'].apply(lambda x: 0 if x <=15 else 1)\n",
    "    data_df['S_2_dt_m_in3']=data_df['S_2_dt_d'].apply(lambda x: 0 if x <=10 else ( 1 if x <=20 else 2))\n",
    "    data_df[\"period_th\"] = data_df[[\"customer_ID\",\"S_2_dt\"]].groupby(\"customer_ID\")[\"S_2_dt\"].rank(\"dense\")\n",
    "    data_df[\"period_reverth\"] = data_df[[\"customer_ID\",\"S_2_dt\"]].groupby(\"customer_ID\")[\"S_2_dt\"].rank(\"dense\",ascending=False)\n",
    "\n",
    "    data_df[\"S_2_yearto2018Mar31\"] = (data_df['S_2_dt'] - pd.Timestamp('2018-03-31'))/np.timedelta64(1, 'Y') #in number\n",
    "    period_list = list(['S_2_dt_y','S_2_dt_m','S_2_dt_d','S_2_dt_wd','S_2_dt_sm','S_2_dt_m_in3','period_th','period_reverth',])\n",
    "    data_df[period_list] = data_df[period_list].astype(\"category\")\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def data_process_itemXXXize(data_df):\n",
    "    global Centreize_items,Standardize_items,Q50diff2\n",
    "    date_gp = data_df.groupby('S_2_dt')#, as_index=False)\n",
    "\n",
    "    #for range int(len(x)*0.5):-int(len(x)*0.5), len(x) should not too low, add max try to fix if issue, ok done\n",
    "    num_list_XXXize = list()\n",
    "    for i, item in enumerate(Centreize_items):\n",
    "        #data_df[item+str('_cen')] = data_df[item].astype('float32').sub(date_gp[item].transform('mean'))\n",
    "        #data_df[item+str('_cen')] = data_df[item].astype('float32').transform(lambda x: (x - x.sort_values()[int(len(x)*0.01):-max(int(len(x)*0.01),1)].mean()))\n",
    "\n",
    "        data_select = data_df[[item,'S_2_dt']]\n",
    "        data_select[item] = data_select[item].astype('float32')\n",
    "        data_df[item+str('_cen')] = data_select.groupby('S_2_dt').transform(lambda x: (x - x.sort_values()[int(len(x)*0.01):-max(int(len(x)*0.01),1)].mean()))\n",
    "        #print(str(i)+':'+ item,end=\"\\r\", flush=True)\n",
    "        num_list_XXXize.append(item+str('_cen'))\n",
    "        \n",
    "    for i, item in enumerate(Standardize_items):\n",
    "        data_select = data_df[[item,'S_2_dt']]\n",
    "        data_select[item] = data_select[item].astype('float32')\n",
    "        data_df[item+str('_std')] = data_select.groupby('S_2_dt').transform(lambda x: (x - x.sort_values()[int(len(x)*0.01):-max(int(len(x)*0.01),1)].mean()\n",
    "        / x.sort_values()[int(len(x)*0.01):-max(int(len(x)*0.01),1)].std()))\n",
    "        #print(str(i)+':'+ item,end=\"\\r\", flush=True)\n",
    "        num_list_XXXize.append(item+str('_std'))\n",
    "    \n",
    "    #Q50diff for D_59\n",
    "    medianvalue = data_df[['D_59','S_2_dt_y','S_2_dt_m']].groupby(['S_2_dt_y','S_2_dt_m']).agg(['median'])\n",
    "    medianvalue.columns = ['D_59_median']\n",
    "    data_df['D_59'] = data_df['D_59'].astype('float32')\n",
    "    Q75value = data_df[['D_59','S_2_dt_y','S_2_dt_m']].groupby(['S_2_dt_y','S_2_dt_m']).quantile(0.75)\n",
    "    Q75value.columns = ['D_59_Q75']\n",
    "    data_df =data_df.merge(medianvalue, how='left', on=['S_2_dt_y','S_2_dt_m'])\n",
    "    data_df =data_df.merge(Q75value, how='left', on=['S_2_dt_y','S_2_dt_m'])\n",
    "    data_df['D_59_Q50std'] = (data_df['D_59'] - data_df['D_59_median'])/(data_df['D_59_Q75']-data_df['D_59_median'])\n",
    "    data_df = data_df.drop(['D_59_median','D_59_Q75'],axis=1)\n",
    "    data_df['D_59_Q50std'] = data_df['D_59_Q50std'].astype('float16')\n",
    "    num_list_XXXize.append('D_59_Q50std')\n",
    "\n",
    "    #Q50diff2\n",
    "    medianvalue = data_df[Q50diff2+['S_2_dt']].groupby(['S_2_dt']).agg(['median'])\n",
    "    medianvalue.columns = ['_'.join(x) for x in medianvalue.columns]\n",
    "\n",
    "    data_df[Q50diff2] = data_df[Q50diff2].astype('float32')\n",
    "    Q75value = data_df[Q50diff2+['S_2_dt']].groupby(['S_2_dt']).quantile(0.75)\n",
    "    #Q75value.columns = [item+'_Q75']\n",
    "    data_df =data_df.merge(medianvalue, how='left', on=['S_2_dt'])\n",
    "    data_df =data_df.merge(Q75value, how='left', on=['S_2_dt'],suffixes=('', '_Q75'))\n",
    "    for item in Q50diff2:\n",
    "        data_df[item+'_Q50std'] = (data_df[item] - data_df[item+'_median'])/(data_df[item+'_Q75']-data_df[item+'_median'])\n",
    "        data_df = data_df.drop([item+'_median',item+'_Q75'],axis=1)\n",
    "        data_df[item+'_Q50std']=data_df[item+'_Q50std'].astype('float16')\n",
    "        num_list_XXXize.append(item+'_Q50std')\n",
    "\n",
    "        num_list_XXXize = list(set(num_list_XXXize))\n",
    "        num_list_XXXize.sort()\n",
    "    return data_df, num_list_XXXize\n",
    "\n",
    "def data_process_fillNA(data_df,num_list):\n",
    "    global cat_list_inModel,fill_0\n",
    "#####fill missing data, num:fill NA with gp mean; cat: fill 'NA'\n",
    "    NA_dict = dict()\n",
    "    for item in cat_list_inModel:\n",
    "        NA_count = data_df[item].isna().sum()\n",
    "        if NA_count >0:\n",
    "            #NA_dict[item+str('-in_cat')] = NA_count\n",
    "            print(item)\n",
    "            data_df[item] = data_df[item].cat.add_categories(\"NA\").fillna(\"NA\")\n",
    "\n",
    "    for i, item in enumerate(fill_0):\n",
    "        NA_count = data_df[item].isna().sum()\n",
    "        if NA_count >0:\n",
    "            #NA_dict[item+str('-in_cat')] = NA_count\n",
    "            data_df[item] = data_df[item].fillna(0)\n",
    "            \n",
    "    for i, item in enumerate(num_list):\n",
    "        NA_count = data_df[item].isna().sum()\n",
    "        if NA_count >0:\n",
    "            #NA_dict[item+str('-in_org')] = NA_count\n",
    "            data_df[item] = data_df[item].astype('float32')\n",
    "            data_df[item] = data_df[item].fillna(data_df[[item,'customer_ID']].groupby('customer_ID')[item].transform('mean'))\n",
    "\n",
    "            NA_count = data_df[item].isna().sum()\n",
    "            if NA_count >0:\n",
    "                #NA_dict[item+str('-af_idm')] = NA_count\n",
    "                data_df[item] = data_df[item].fillna(data_df[[item,'S_2_dt']].groupby('S_2_dt')[item].transform('mean'))\n",
    "                \n",
    "                NA_count = data_df[item].isna().sum()\n",
    "                if NA_count >0:\n",
    "                    #NA_dict[item+str('-af_S2m')] = NA_count\n",
    "                    data_df[item] = data_df[item].fillna(data_df[item].mean()) \n",
    "        data_df[item] = data_df[item].astype('float16')\n",
    "        print('NA'+str(i)+':'+ item, end = \"\\r\", flush=True)\n",
    "    return data_df\n",
    "\n",
    "def dataP_basestat(data_df,num_list):\n",
    "    num_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].agg(['mean', 'std','first','last', 'min', 'max', ])\n",
    "    num_agg.columns = ['_'.join(x) for x in num_agg.columns]\n",
    "    for item in num_list:\n",
    "        num_agg[item+'_lagmean'] = num_agg[item+'_last']  - num_agg[item+'_mean'] \n",
    "    num_agg = num_agg.astype('float16')\n",
    "    num_agg.reset_index(inplace = True)\n",
    "    return num_agg\n",
    "\n",
    "def dataP_avgstat(data_df,num_list):\n",
    "    output_df = pd.DataFrame(index = data_df['customer_ID'].unique()).rename_axis(index = 'customer_ID')\n",
    "    for j in [3,6,]:\n",
    "        num_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].nth(range(j)).groupby(\"customer_ID\").mean()\n",
    "        std_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].nth(range(j)).groupby(\"customer_ID\").std()\n",
    "        data_last = data_df.groupby(\"customer_ID\")[num_list].last() #lag_org\n",
    "        diff_agg = pd.DataFrame()\n",
    "        diff_agg = data_last - num_agg\n",
    "\n",
    "        num_agg.columns = [str(col) + '_f'+str(j)+'avg' for col in num_agg.columns]\n",
    "        diff_agg.columns = [str(col) + '_lag_diff_f'+str(j)+'avg' for col in diff_agg.columns]\n",
    "        std_agg.columns = [str(col) + '_f'+str(j)+'std' for col in std_agg.columns]\n",
    "\n",
    "        output_df = output_df.join(num_agg,on='customer_ID')\n",
    "        output_df = output_df.join(diff_agg,on='customer_ID')\n",
    "        output_df = output_df.join(std_agg,on='customer_ID')\n",
    "\n",
    "    for j in [-3,-6,]:\n",
    "        num_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].nth(range(-1,j-1,-1)).groupby(\"customer_ID\").mean()\n",
    "        std_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].nth(range(-1,j-1,-1)).groupby(\"customer_ID\").std()\n",
    "        data_last = data_df.groupby(\"customer_ID\")[num_list].last() #lag_org\n",
    "        diff_agg = pd.DataFrame()\n",
    "        diff_agg = data_last - num_agg\n",
    "\n",
    "        num_agg.columns = [str(col) + '_f'+str(j)+'avg' for col in num_agg.columns]\n",
    "        diff_agg.columns = [str(col) + '_lag_diff_f'+str(j)+'avg' for col in diff_agg.columns]\n",
    "        std_agg.columns = [str(col) + '_f'+str(j)+'std' for col in std_agg.columns]\n",
    "\n",
    "        output_df = output_df.join(num_agg,on='customer_ID')\n",
    "        output_df = output_df.join(diff_agg,on='customer_ID')\n",
    "        output_df = output_df.join(std_agg,on='customer_ID')\n",
    "    num_agg.reset_index(inplace = True)\n",
    "    return output_df\n",
    "\n",
    "def dataP_catstat(data_df,cat_list):\n",
    "    cat_agg = data_df.groupby(\"customer_ID\")[cat_list].agg([ 'last'])#'count',\n",
    "    cat_agg.columns = ['_'.join(x) for x in cat_agg.columns]\n",
    "    cat_agg.reset_index(inplace = True)\n",
    "    return cat_agg\n",
    "\n",
    "\n",
    "\n",
    "def data_process_PCA(data_df,num_list,n_feature,pca=PCA()):\n",
    "    #PCA feature #train need to apply same pca to test!\n",
    "    if pca == PCA():\n",
    "        pca = PCA(n_feature)\n",
    "        pca.fit(data_df[num_list])\n",
    "    pca_feature = pca.transform(data_df[num_list])\n",
    "    r,l = pca_feature.shape\n",
    "    col_list = list()\n",
    "    for i in range(l):\n",
    "        col_list.append(\"PAC_\"+str(i))\n",
    "\n",
    "    principalDf = pd.DataFrame(data = pca_feature, columns = col_list)\n",
    "    #data_df = pd.concat([data_df,principalDf],axis=1)\n",
    "    return  principalDf, pca\n",
    "\n",
    "\n",
    "def save_df(data_df,dataset,Version):\n",
    "    global cat_list,Huge_data_path\n",
    "    data_df[cat_list] = data_df[cat_list].astype(str)\n",
    "    data_df.to_feather(Huge_data_path + Version +'_'+dataset+'.ftr') \n",
    "    return data_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Old code\n",
    "\n",
    "def dataP_single_lag(data_df,num_list):\n",
    "    data_df = data_df.sort_values('S_2_dt')# early to latest #,ascending = False) #latest to early \n",
    "    data_df[num_list] = data_df[num_list].astype('float32')\n",
    "    data_last = data_df.groupby(\"customer_ID\")[num_list].last() #lag_org\n",
    "    data_frist = data_df.groupby(\"customer_ID\")[num_list].first() #for NA fill\n",
    "    order_agg = pd.DataFrame(index = data_last.index)\n",
    "    #for k in range(1,7):\n",
    "    for k in [1,3,6]:\n",
    "        print(k, end = \"\\r\", flush=True)\n",
    "        data_n = pd.DataFrame(index=data_last.index)\n",
    "        data_n = data_n.join(data_df.groupby('customer_ID').nth(-k-1)[num_list],on='customer_ID')\n",
    "        data_n[data_n.isnull()] = data_frist\n",
    "        col_list = data_n.columns\n",
    "\n",
    "        data_n[num_list] = data_last[num_list] - data_n[num_list]\n",
    "        data_n.columns =  [str(col) + '_lag'+str(k)+'_sub' for col in col_list]\n",
    "        order_agg = order_agg.join(data_n,on='customer_ID')\n",
    "\n",
    "        data_n.columns =  col_list\n",
    "        data_n[num_list] = data_n[num_list]/data_last[num_list]\n",
    "        data_n.columns =  [str(col) + '_lag'+str(k)+'_prop' for col in col_list]\n",
    "        order_agg = order_agg.join(data_n,on='customer_ID')\n",
    "\n",
    "    data_n = data_frist\n",
    "    k='f'\n",
    "    col_list = data_n.columns\n",
    "    data_n[num_list] = data_last[num_list] - data_n[num_list]\n",
    "    data_n.columns =  [str(col) + '_lag'+str(k)+'_sub' for col in col_list]\n",
    "    order_agg = order_agg.join(data_n,on='customer_ID')\n",
    "\n",
    "    data_n.columns =  col_list\n",
    "    data_n[num_list] = data_n[num_list]/data_last[num_list]\n",
    "    data_n.columns =  [str(col) + '_lag'+str(k)+'_prop' for col in col_list]\n",
    "    order_agg = order_agg.join(data_n,on='customer_ID')\n",
    "    order_agg = order_agg.astype('float16').reset_index()\n",
    "    return order_agg\n",
    "\n",
    "def dataP_slope(data_df,Slope_items):\n",
    "    review_T = pd.DataFrame()\n",
    "    for i, item in enumerate( Slope_items):\n",
    "        for col in data_df.columns:\n",
    "            if item == col or item+'_' in col:\n",
    "                review_T[col+'_slope'] = data_df[['customer_ID','S_2_yearto2018Mar31',col]].groupby('customer_ID').apply(lambda x: linregress(x['S_2_yearto2018Mar31'],x[item])[0])\n",
    "                print('Slope-'+str(i)+':'+ col, end = \"\\r\", flush=True)\n",
    "    return review_T\n",
    "\n",
    "def dataP_stat(data_df,num_list,cat_list):\n",
    "    num_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].agg(['mean', 'std', 'min', 'max', ])\n",
    "    num_agg.columns = ['_'.join(x) for x in num_agg.columns]\n",
    "    num_agg.reset_index(inplace = True)\n",
    "\n",
    "    order_agg = data_df.sort_values('S_2_dt').groupby(\"customer_ID\")[num_list].agg([nth_1,nth_2,nth_3,nth_6, 'first'])\n",
    "    order_agg.columns = ['_'.join(x) for x in num_agg.columns]\n",
    "    order_agg.reset_index(inplace = True)\n",
    "    drop_list = list()\n",
    "    for col in order_agg:\n",
    "        if 'nth_1' in col and col.replace('nth_1', 'first') in order_agg:\n",
    "            order_agg[col + '_lagf_sub'] = order_agg[col] - order_agg[col.replace('nth_1', 'first')]\n",
    "            order_agg[col + '_lagf_prop'] = (order_agg[col] - order_agg[col.replace('nth_1', 'first')])/order_agg[col]\n",
    "        for k in range(2,7):\n",
    "            if 'nth_1' in col and col.replace('nth_1', 'nth_'+str(k)) in order_agg:\n",
    "                order_agg[col + '_lag'+str(k)+'_sub'] = order_agg[col] - order_agg[col.replace('nth_1', 'nth_'+str(k))]\n",
    "                order_agg[col + '_lag'+str(k)+'_prop'] = (order_agg[col] - order_agg[col.replace('nth_1', 'nth_'+str(k))])/order_agg[col]\n",
    "                drop_list.append(col.replace('nth_1', 'nth_'+str(k)))\n",
    "    order_agg = order_agg.drop(drop_list,axis=1)\n",
    "            \n",
    "    \n",
    "    # cat_agg = data_df.groupby(\"customer_ID\")[cat_list].agg([ 'first', 'last', 'nunique'])#'count',\n",
    "    # cat_agg.columns = ['_'.join(x) for x in cat_agg.columns]\n",
    "    # cat_agg.reset_index(inplace = True)\n",
    "    #df = data_df[data_df['period_reverth']==1][['customer_ID','period']]\n",
    "    # df = num_agg.merge(cat_agg, how = 'inner', on = 'customer_ID')\n",
    "    #Slope\n",
    "    #for item in Slope_items:\n",
    "    #   review_T[item+'_slope'] = train_data_df[['customer_ID','S_2_yearto2018Mar31',item]].groupby('customer_ID').apply(lambda x: linregress(x['S_2_yearto2018Mar31'],x[item])[0])\n",
    "\n",
    "    return order_agg\n",
    "\n",
    "\n",
    "def data_process_Preioddiff(data_df,num_list):\n",
    "    \n",
    "    for j,item in enumerate(num_list):\n",
    "        review_T = data_df[[item,'customer_ID','period_reverth','period_th']]\n",
    "        #review_T[item] = review_T[item].astype('float32')\n",
    "        review_T_v2 = review_T[review_T['period_reverth']==1].rename(columns={item:'last'}).drop(['period_reverth','period_th'],axis=1)\n",
    "        review_T_temp = review_T[(review_T['period_th']==1)].rename(columns={item:'first'}).drop(['period_reverth','period_th'],axis=1)\n",
    "        review_T_v2 = review_T_v2.join(review_T_temp.set_index('customer_ID'),on='customer_ID')#,left_on = 'customer_ID')\n",
    "        review_T_v2[str(item)+'_diff_st2ed'] = review_T_v2['first'] - review_T_v2['last']\n",
    "        review_T_v2[str(item)+'_diff%_st2ed'] = (review_T_v2['first'] - review_T_v2['last'])/review_T_v2['last']\n",
    "        data_df = data_df.join(review_T_v2.drop(['last','first'],axis=1).set_index('customer_ID'),on='customer_ID')#,left_on = 'customer_ID')\n",
    "        new_list = list([str(item)+'_diff_st2ed',str(item)+'_diff%_st2ed'])\n",
    "        data_df[new_list] = data_df[new_list].astype('float16')\n",
    "        print('preioddiff'+str(j)+':'+ item, end = \"\\r\", flush=True)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def baseline_get_difference(data, num_features):\n",
    "    df1 = []\n",
    "    customer_ids = []\n",
    "    for customer_id, df in tqdm(data.groupby(['customer_ID'])):\n",
    "        # Get the differences\n",
    "        diff_df1 = df[num_features].diff(1).iloc[[-1]].values.astype(np.float32)\n",
    "        # Append to lists\n",
    "        df1.append(diff_df1)\n",
    "        customer_ids.append(customer_id)\n",
    "    # Concatenate\n",
    "    df1 = np.concatenate(df1, axis = 0)\n",
    "    # Transform to dataframe\n",
    "    df1 = pd.DataFrame(df1, columns = [col + '_diff1' for col in df[num_features].columns])\n",
    "    # Add customer id\n",
    "    df1['customer_ID'] = customer_ids\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = ['PCA_1', 'D_115_lag1_sub', 'D_126_Woeslope', 'S_9_std_f-3avg',\n",
    "       'S_5_std_last', 'D_116_Woeslope', 'S_5_std_min', 'R_27_f-3std',\n",
    "       'S_9_std_f-6std', 'S_8_std_last', 'D_60_cen_last', 'D_120_Woeslope',\n",
    "       'D_121_f6std', 'D_119_lag1_prop', 'D_121_std', 'D_55_cen_lag3_sub',\n",
    "       'D_119_f6std', 'D_124_slope', 'R_27_lag_diff_f-3avg', 'PCA_0',\n",
    "       'S_9_std_f-3std', 'S_2_dt_wd_last_WoE', 'R_27_f-6std', 'S_9_min',\n",
    "       'D_115_f6std', 'D_56_lag1_sub', 'D_73_std_min', 'D_126_na_mean',\n",
    "       'D_125_lag1_sub', 'D_73_std_max', 'R_27_lag_diff_f-6avg',\n",
    "       'D_126_std_WoE', 'PCA_2', 'D_121_f-6std', 'R_27_na_mean',\n",
    "       'D_125_lag_diff_f-3avg', 'D_61_cen_min', 'D_123_lag1_sub',\n",
    "       'S_9_std_last', 'D_121_lag3_sub', 'R_27_lag3_sub', 'R_27_f-3avg',\n",
    "       'R_9_first', 'D_115_f-3std', 'S_9_std_f-6avg', 'D_119_std',\n",
    "       'D_126_nunique', 'S_9_f-3avg', 'D_120_mean_WoE', 'S_8_std_min',\n",
    "       'D_73_std_lagf_sub', 'D_114_Woeslope', 'D_73_std_first', 'R_27_lagmean',\n",
    "       'D_91_lag1_sub', 'D_121_f-3std', 'D_121_f3std', 'S_9_std_max',\n",
    "       'D_73_std_mean', 'D_115_lag1_prop', 'D_122_lag1_prop',\n",
    "       'D_73_std_lag6_sub', 'D_55_lag3_sub', 'S_9_f-6std', 'D_73_std_lagmean',\n",
    "       'D_120_std_WoE', 'R_27_f6std', 'D_121_lag_diff_f-6avg', 'S_18_last',\n",
    "       'D_55_cen_f6avg', 'D_126_first_WoE', 'D_55_cen_f-6std',\n",
    "       'D_73_std_lagf_prop', 'R_26_lagmean', 'D_124_lag_diff_f6avg',\n",
    "       'D_45_lag6_sub', 'R_27_lag6_prop', 'D_73_min', 'D_115_std', 'R_27_std',\n",
    "       'S_8_std_f-3std', 'D_113_lag_diff_f-6avg', 'D_121_lagf_sub',\n",
    "       'S_9_f-3std', 'R_9_std_std', 'D_55_f-6std', 'P_4_max', 'S_27_f6avg',\n",
    "       'S_18_lag_diff_f-6avg', 'D_119_f3avg', 'S_18_f-3avg', 'D_73_std',\n",
    "       'R_27_lag3_prop', 'D_55_mean', 'D_118_lag_diff_f-3avg',\n",
    "       'S_2_dt_m_in3_lagmean_WoE', 'D_115_lag3_prop',\n",
    "       'S_9_std_lag_diff_f-3avg', 'S_8_max', 'D_123_f6std',\n",
    "       'S_9_std_mean', 'D_118_lag_diff_f-6avg', 'D_119_lag_diff_f-6avg',\n",
    "       'D_63_Woeslope', 'D_55_cen_lagmean', 'D_55_lag_diff_f6avg',\n",
    "       'D_121_lag_diff_f6avg', 'D_55_cen_lag_diff_f6avg', 'D_118_lagmean',\n",
    "       'S_9_na_mean', 'D_118_lag3_sub', 'D_119_lag_diff_f6avg',\n",
    "       'S_8_std_f-6avg', 'S_9_mean', 'S_9_std_lag_diff_f-6avg',\n",
    "       'D_119_lag3_sub',]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f2f08eedc2a88ebcb06b7e0c78e7030e176dfefbb319999fb4d7cf1f2178b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
